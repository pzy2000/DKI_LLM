from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
from transformers import AutoModelForCausalLM
from vllm.lora.request import LoRARequest
class Qwen3_8B:
    """
    Qwen3-8B inference using vLLM backend.
    """

    def __init__(self, model_path: str,tp=4,lora=False):
        # Load tokenizer normally
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

        self.lora=lora
        if self.lora:
            self.lora_local_path=f"{model_path}/lora_adapter"
            print("Using LoRA-adapted model from:", self.lora_local_path)
            self.lora_request=LoRARequest("self_adapter_v1", 1, lora_local_path=self.lora_local_path)

        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=tp,
            enable_prefix_caching=True,   # âœ… å¼€å¯ prefix caching
            gpu_memory_utilization=0.8,   # âœ… å¸¸ç”¨æ¨èå€¼
            enable_lora=self.lora,            # âœ… å¯ç”¨ LoRA
            max_lora_rank=32,
        )

        print("ğŸš€ Qwen3â€‘8B loaded using vLLM.")
    def generate_batch(
        self,
        system_prompts: list[str],
        user_prompts: list[str],
        max_new_tokens=2048,
        thinking=False,
    ):
        assert len(system_prompts) == len(user_prompts)

        messages_list = []
        for sys_p, usr_p in zip(system_prompts, user_prompts):
            messages = [
                {"role": "system", "content": sys_p},
                {"role": "user", "content": usr_p},
            ]
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=thinking,
            )
            messages_list.append(prompt)

        sampling_params = SamplingParams(
            max_tokens=max_new_tokens,
            temperature=0.6 if thinking else 0.7,
            top_p=0.95 if thinking else 0.8,
            top_k=20,
            min_p=0,
        )

        outputs = self.llm.generate(
            prompts=messages_list,
            sampling_params=sampling_params,
            lora_request=self.lora_request if self.lora else None,
        )

        results = []
        for out in outputs:
            text = out.outputs[0].text.strip()
            results.append(text)

        return results

    # -----------------------------------------------------
    # Chat-style generation (system + user)
    # -----------------------------------------------------
    def generate(self, system_prompt: str, user_prompt: str, max_new_tokens=2048, thinking=False):

        # print("===== System Prompt =====")
        # print(system_prompt)

        # print("===== User Prompt =====")
        # print(user_prompt)

        # ğŸ”¥ æ„é€  Chat Template
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]


        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=thinking  # æ§åˆ¶æ˜¯å¦æ·»åŠ  <think> æ ‡ç­¾
        )

        # âœ… æŒ‰ç…§æ˜¯å¦æ€ç»´æ¨¡å¼ï¼Œä½¿ç”¨ä¸åŒ Sampling å‚æ•°
        if thinking:
            sampling_params = SamplingParams(
                max_tokens=max_new_tokens,
                temperature=0.6,
                top_p=0.95,
                top_k=20,
                min_p=0
            )
        else:
            sampling_params = SamplingParams(
                max_tokens=max_new_tokens,
                temperature=0.7,
                top_p=0.8,
                top_k=20,
                min_p=0
            )

        # ğŸ”® æ‰§è¡Œ vLLM ç”Ÿæˆ

        outputs = self.llm.generate(
            prompt,
            sampling_params=sampling_params,
            lora_request=self.lora_request if self.lora else None,
        )

        full_output = outputs[0].outputs[0].text.strip()

        # ğŸ¤” æ˜¯å¦åŒ…å«æ€ç»´æ¨¡å¼è¾“å‡º
        if thinking:
            THINK_END_TOKEN = "</think>"
            if THINK_END_TOKEN in full_output:
                thinking_content, final = full_output.split(THINK_END_TOKEN, 1)
                thinking_content = thinking_content.replace("<think>", "").strip()
                return thinking_content, final.strip()

            # fallbackï¼šæ²¡ç”Ÿæˆæ€è€ƒæ®µ
            return "", full_output
        else:
            # éæ€ç»´æ¨¡å¼ï¼Œç›´æ¥è¿”å›
            return "", full_output

